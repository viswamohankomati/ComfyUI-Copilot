[
    {
        "name": "æ·»åŠ LoRA",
        "description": "åœ¨ç°æœ‰å·¥ä½œæµä¸­æ·»åŠ LoRAèŠ‚ç‚¹",
        "content": "åœ¨ç°æœ‰å·¥ä½œæµä¸­æ·»åŠ LoRAèŠ‚ç‚¹ï¼Œç¡®ä¿ä¸ç°æœ‰æ¨¡å‹å’Œæç¤ºè¯èŠ‚ç‚¹æ­£ç¡®è¿æ¥\n    åœ¨checkpointèŠ‚ç‚¹åæ·»åŠ LoRAèŠ‚ç‚¹ã€‚\n    {\n      \"1\": {\n         \"inputs\": {\n            \"lora_name\": \"DOG.safetensors\",\n            \"strength_model\": 1,\n            \"strength_clip\": 1,\n            \"model\": [\n            \"2\",\n            0\n            ],\n            \"clip\": [\n            \"2\",\n            1\n            ]\n         },\n         \"class_type\": \"LoraLoader\",\n         \"_meta\": {\n            \"title\": \"Load LoRA\"\n         }\n      }"
    },
    {
        "name": "åå¤„ç†å¢å¼º",
        "description": "åå¤„ç†å¢å¼ºï¼Œä¾‹å¦‚åœ¨Preview Imageæˆ–Save ImageèŠ‚ç‚¹åæ·»åŠ é«˜æ¸…æ”¾å¤§åŠŸèƒ½ï¼ˆå¦‚Real-ESRGANã€ESRGANç­‰ï¼‰; æˆ–è€…æ·»åŠ å›¾åƒç¼©æ”¾èŠ‚ç‚¹",
        "content": "- åœ¨Preview Imageæˆ–Save ImageèŠ‚ç‚¹åæ·»åŠ é«˜æ¸…æ”¾å¤§åŠŸèƒ½ï¼ˆå¦‚Real-ESRGANã€ESRGANç­‰ï¼‰\n       - æ·»åŠ å›¾åƒç¼©æ”¾èŠ‚ç‚¹\n       {\n  \"1\": {\n    \"inputs\": {\n      \"width\": 512,\n      \"height\": 512,\n      \"interpolation\": \"nearest\",\n      \"method\": \"stretch\",\n      \"condition\": \"always\",\n      \"multiple_of\": 0\n    },\n    \"class_type\": \"ImageResize+\",\n    \"_meta\": {\n      \"title\": \"ğŸ”§ Image Resize\"\n    }\n  }\n}\n       - æ·»åŠ å›¾åƒå°ºå¯¸è°ƒæ•´èŠ‚ç‚¹\n       {\n  \"2\": {\n    \"inputs\": {\n      \"aspect_ratio\": \"original\",\n      \"proportional_width\": 2,\n      \"proportional_height\": 1,\n      \"fit\": \"letterbox\",\n      \"method\": \"lanczos\",\n      \"round_to_multiple\": \"8\",\n      \"scale_to_longest_side\": false,\n      \"longest_side\": 1024\n    },\n    \"class_type\": \"LayerUtility: ImageScaleByAspectRatio\",\n    \"_meta\": {\n      \"title\": \"LayerUtility: ImageScaleByAspectRatio\"\n    }\n  }\n}\n   -æ·»åŠ å›¾åƒæ”¾å¤§èŠ‚ç‚¹\n  \"11\": {\n    \"inputs\": {\n      \"width\": 512,\n      \"height\": 512,\n      \"upscale_method\": \"nearest-exact\",\n      \"keep_proportion\": false,\n      \"divisible_by\": 2,\n      \"crop\": \"disabled\",\n      \"image\": [\n        \"12\",\n        0\n      ]\n    },\n    \"class_type\": \"ImageResizeKJ\",\n    \"_meta\": {\n      \"title\": \"Resize Image\"\n    }\n  },\n   -æ·»åŠ VAEdecodeèŠ‚ç‚¹\n  \"12\": {\n    \"inputs\": {},\n    \"class_type\": \"VAEDecode\",\n    \"_meta\": {\n      \"title\": \"VAE Decode\"\n    }\n  }\n}"
    },
    {
        "name": "æç¤ºè¯ä¼˜åŒ–",
        "description": "ä¿®æ”¹ç°æœ‰æç¤ºè¯èŠ‚ç‚¹çš„å†…å®¹(æç¤ºè¯åº”è¯¥åœ¨(CLIP Text Encode Promptæˆ–Text _OèŠ‚ç‚¹å†…ç¼–è¾‘); æ·»åŠ å•ç‹¬çš„æç¤ºè¯è¾“å…¥èŠ‚ç‚¹",
        "content": "{\n  \"12\": {\n    \"inputs\": {\n      \"text\": [\n        \"13\",\n        0\n      ]\n    },\n    \"class_type\": \"CLIPTextEncode\",\n    \"_meta\": {\n      \"title\": \"CLIP Text Encode (Prompt)\"\n    }\n  },\n  \"13\": {\n    \"inputs\": {\n      \"delimiter\": \", \",\n      \"clean_whitespace\": \"true\",\n      \"text_a\": [\n        \"15\",\n        0\n      ],\n      \"text_b\": [\n        \"16\",\n        0\n      ]\n    },\n    \"class_type\": \"Text Concatenate\",\n    \"_meta\": {\n      \"title\": \"Text Concatenate\"\n    }\n  },\n  \"15\": {\n    \"inputs\": {\n      \"text\": \"\"\n    },\n    \"class_type\": \"Text _O\",\n    \"_meta\": {\n      \"title\": \"Text _O\"\n    }\n  },\n  \"16\": {\n    \"inputs\": {\n      \"text\": \"\"\n    },\n    \"class_type\": \"Text _O\",\n    \"_meta\": {\n      \"title\": \"Text _O\"\n    }\n  }\n}"
    },
    {
        "name": "å›¾åƒåæ¨",
        "description": "ç†è§£å›¾ç‰‡ä¿¡æ¯å¹¶ç”¨æ–‡å­—è¡¨è¾¾å‡ºæ¥",
        "content": "æ·»åŠ å›¾åƒåæ¨èŠ‚ç‚¹ï¼ˆå¦‚CLIP Interrogatorï¼‰\n{\n  \"11\": {\n    \"inputs\": {\n      \"prompt_mode\": \"fast\",\n      \"image_analysis\": \"off\"\n    },\n    \"class_type\": \"ClipInterrogator\",\n    \"_meta\": {\n      \"title\": \"Clip Interrogator â™¾ï¸Mixlab\"\n    }\n  }\n}\n       - æ·»åŠ æ›´å¤æ‚æˆ–è€…æ›´å¥½ç”¨çš„å›¾åƒåæ¨èŠ‚ç‚¹(åŠ è½½å›¾ç‰‡ä½¿ç”¨florence2è¿›è¡Œåæ¨)\n       {\n  \"6\": {\n    \"inputs\": {\n      \"image\": \"06.JPG\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"$image.image!:The image to analyze, must be a url\"\n    }\n  },\n  \"10\": {\n    \"inputs\": {\n      \"model\": \"microsoft/Florence-2-large\",\n      \"precision\": \"fp16\",\n      \"attention\": \"sdpa\"\n    },\n    \"class_type\": \"DownloadAndLoadFlorence2Model\",\n    \"_meta\": {\n      \"title\": \"DownloadAndLoadFlorence2Model\"\n    }\n  },\n  \"11\": {\n    \"inputs\": {\n      \"text_input\": \"\",\n      \"task\": \"more_detailed_caption\",\n      \"fill_mask\": true,\n      \"keep_model_loaded\": false,\n      \"max_new_tokens\": 1024,\n      \"num_beams\": 3,\n      \"do_sample\": true,\n      \"output_mask_select\": \"\",\n      \"seed\": 1098631327477633,\n      \"image\": [\n        \"6\",\n        0\n      ],\n      \"florence2_model\": [\n        \"10\",\n        0\n      ]\n    },\n    \"class_type\": \"Florence2Run\",\n    \"_meta\": {\n      \"title\": \"Florence2Run\"\n    }\n  },\n  \"18\": {\n    \"inputs\": {\n      \"anything\": [\n        \"11\",\n        2\n      ]\n    },\n    \"class_type\": \"easy showAnything\",\n    \"_meta\": {\n      \"title\": \"Show Any\"\n    }\n  },\n  \"20\": {\n    \"inputs\": {\n      \"value\": \"Generate high-quality text descriptions from images using local Florence model.\\n    \\n    Main use cases:\\n    1. **Reverse image prompt generation**:\\n       - When users upload an image and want to get prompts for AI art generation\\n       - Analyzes visual elements, style, composition, etc. to generate prompts for Stable Diffusion, DALL-E, and other models\\n       - In this case, return the tool's raw output directly to users without any modification or summary\\n       - Users can directly use these prompts for image generation\\n    2. **Image content understanding**:\\n       - When users ask about specific content, objects, scenes, people, etc. in the image\\n       - Need to understand the semantic content of the image and answer users' specific questions\\n       - In this case, combine tool output with conversation context to give users contextually appropriate natural responses\\n       - Don't return raw output directly, but provide targeted replies based on understanding results\"\n    },\n    \"class_type\": \"PrimitiveStringMultiline\",\n    \"_meta\": {\n      \"title\": \"MCP\"\n    }\n  }\n}"
    },
    {
        "name": "sdxlç‰¹å¾ä¿æŒ",
        "description": "å½“ç”¨æˆ·åœ¨ä½¿ç”¨sdxlå·¥ä½œæµå¹¶éœ€æåŠç‰¹å¾ä¿æŒæ—¶ä¼˜å…ˆæ¨èè¯¥ç±»å·¥ä½œæµï¼Œæ—¨åœ¨ä¿æŒå›¾åƒçš„ç‰¹å¾æå–ã€ä¿æŒä¸»ä½“ç»“æ„ä¸å˜ã€ä¿æŒä¸»ä½“å½¢çŠ¶ã€æ§åˆ¶äººç‰©è§’è‰²å§¿æ€ã€ä¿æŒåœºæ™¯çš„æ·±åº¦å’Œç©ºé—´ç»“æ„ã€åŸºäºçº¿ç¨¿æˆ–è€…è‰å›¾ç”Ÿæˆå›¾åƒç­‰åœºæ™¯ä¸‹ï¼Œå¯ä»¥ç”¨æ·»åŠ ControlNetèŠ‚ç‚¹æ¥å®ç°",
        "content": "**ControlNeté›†æˆ**ï¼š(preprocessorå¯é€‰cannyã€depthç­‰å‚æ•°,æ¨¡å‹æ ·å¼é€‰æ‹©ï¼ŒControlnetå¼€å§‹ã€ç»“æŸæƒé‡)\n       {\n  \"22\": {\n    \"inputs\": {\n      \"strength\": 1,\n      \"start_percent\": 0,\n      \"end_percent\": 1,\n      \"control_net\": [\n        \"23\",\n        0\n      ],\n      \"image\": [\n        \"24\",\n        0\n      ]\n    },\n    \"class_type\": \"ControlNetApplyAdvanced\",\n    \"_meta\": {\n      \"title\": \"Apply ControlNet\"\n    }\n  },\n  \"23\": {\n    \"inputs\": {\n      \"control_net_name\": \"ControlNet-Standard-Lineart-for-SDXL.safetensors\"\n    },\n    \"class_type\": \"ControlNetLoader\",\n    \"_meta\": {\n      \"title\": \"Load ControlNet Model\"\n    }\n  },\n  \"24\": {\n    \"inputs\": {\n      \"preprocessor\": \"none\",\n      \"resolution\": 512\n    },\n    \"class_type\": \"AIO_Preprocessor\",\n    \"_meta\": {\n      \"title\": \"AIO Aux Preprocessor\"\n    }\n  }\n}"
    },
    {
        "name": "fluxé£æ ¼ç‰¹å¾è¿ç§»",
        "description": "å½“ç”¨æˆ·åœ¨ä½¿ç”¨fluxçš„å·¥ä½œæµå¹¶æåŠé£æ ¼ç‰¹å¾è¿ç§»æ—¶ä¼˜å…ˆæ¨èè¯¥èŠ‚ç‚¹ï¼Œè¯¥ç±»èŠ‚ç”¨é€”ä¸»è¦åº”ç”¨äºfluxå·¥ä½œæµçš„å›¾ç‰‡é£æ ¼ç‰¹å¾è¿ç§»å’Œæå–ï¼Œé£æ ¼è½¬æ¢ï¼Œä¸¤å¼ å›¾ç‰‡å‚è€ƒè¿ç§»ï¼Œé‡ç»˜é®ç½©åŒºåŸŸæˆ–é‡ç»˜èƒŒæ™¯ï¼Œä½¿ç”¨æ—¶è¯·å…ˆåˆ†ææ˜¯å¦ä¸ºfluxå·¥ä½œæµ",
        "content": "{\n  \"1\": {\n    \"inputs\": {\n      \"downsampling_factor\": 3,\n      \"downsampling_function\": \"area\",\n      \"mode\": \"center crop (square)\",\n      \"weight\": 1,\n      \"autocrop_margin\": 0.1,\n      \"style_model\": [\n        \"2\",\n        0\n      ],\n      \"clip_vision\": [\n        \"3\",\n        0\n      ]\n    },\n    \"class_type\": \"ReduxAdvanced\",\n    \"_meta\": {\n      \"title\": \"ReduxAdvanced\"\n    }\n  },\n  \"2\": {\n    \"inputs\": {\n      \"style_model_name\": \"flux1-redux-dev.safetensors\"\n    },\n    \"class_type\": \"StyleModelLoader\",\n    \"_meta\": {\n      \"title\": \"Load Style Model\"\n    }\n  },\n  \"3\": {\n    \"inputs\": {\n      \"clip_name\": \"sigclip_vision_patch14_384.safetensors\"\n    },\n    \"class_type\": \"CLIPVisionLoader\",\n    \"_meta\": {\n      \"title\": \"Load CLIP Vision\"\n    }\n  }\n}"
    },
    {
        "name": "sdxlç‰¹å¾è¿ç§»",
        "description": "å½“ç”¨æˆ·åœ¨ä½¿ç”¨sdxlå·¥ä½œæµå¹¶æåŠç‰¹å¾è¿ç§»æ—¶ä¼˜å…ˆæ¨èè¯¥ç±»å·¥ä½œæµï¼Œæ—¨åœ¨ä¿æŒå›¾åƒçš„ç‰¹å¾æå–ã€é¢œè‰²æå–,ä¿æŒä¸»ä½“ä¸å˜é‡ç»˜èƒŒæ™¯æˆ–æ”¹å˜é®ç½©åŒºåŸŸå†…å®¹ï¼Œå¯ä»¥ç”¨æ·»åŠ IPAdapter AdvancedèŠ‚ç‚¹æ¥å®ç°",
        "content": "**IPAdapteré›†æˆ**ï¼š(weight typeå¯é€‰linearã€ease in-outç­‰å‚æ•°,weightå¯é€‰0.1-1å¼€å§‹ã€ç»“æŸæƒé‡å¯é€‰0.1-1) {\n  \"1\": {\n    \"inputs\": {\n      \"weight\": 1,\n      \"weight_type\": \"linear\",\n      \"combine_embeds\": \"concat\",\n      \"start_at\": 0,\n      \"end_at\": 1,\n      \"embeds_scaling\": \"V only\",\n      \"ipadapter\": [\n        \"2\",\n        0\n      ],\n      \"clip_vision\": [\n        \"3\",\n        0\n      ]\n    },\n    \"class_type\": \"IPAdapterAdvanced\",\n    \"_meta\": {\n      \"title\": \"IPAdapter Advanced\"\n    }\n  },\n  \"2\": {\n    \"inputs\": {\n      \"ipadapter_file\": \"ip-adapter_sdxl_vit-h.safetensors\"\n    },\n    \"class_type\": \"IPAdapterModelLoader\",\n    \"_meta\": {\n      \"title\": \"IPAdapter Model Loader\"\n    }\n  },\n  \"3\": {\n    \"inputs\": {\n      \"clip_name\": \"CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors\"\n    },\n    \"class_type\": \"CLIPVisionLoader\",\n    \"_meta\": {\n      \"title\": \"Load CLIP Vision\"\n    }\n  }\n}"
    },
    {
        "name": "æ‰©å›¾",
        "description": "ç»™æˆ‘ä¸€ä¸ªæ‰©å›¾é“¾è·¯(åŒ…æ‹¬å›¾åƒåŠ è½½å’Œå›¾åƒè¾“å‡º)",
        "content": "{\n  \"1\": {\n    \"inputs\": {\n      \"image\": \"01.JPG\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"$image.image!:The image to analyze, must be a url\"\n    }\n  },\n  \"2\": {\n    \"inputs\": {\n      \"model\": \"runwayml/stable-diffusion-xl-base-1.0\",\n      \"precision\": \"fp16\",\n      \"attention\": \"sdpa\"\n    },\n    \"class_type\": \"DownloadAndLoadModel\",\n    \"_meta\": {\n      \"title\": \"DownloadAndLoadModel\"\n    }\n  },\n  \"3\": {\n    \"inputs\": {\n      \"prompt\": \"A beautiful girl with long hair and a white dress\",\n      \"image\": [\n        \"1\",\n        0\n      ],\n      \"model\": [\n        \"2\",\n        0\n      ]\n    },\n    \"class_type\": \"StableDiffusionXLRun\",\n    \"_meta\": {\n      \"title\": \"StableDiffusionXLRun\"\n    }\n  },\n  \"4\": {\n    \"inputs\": {\n      \"image\": [\n        \"3\",\n        0\n      ]\n    },\n    \"class_type\": \"easy showImage\",\n    \"_meta\": {\n      \"title\": \"Show Image\"\n    }\n  }\n}"
    },
    {
        "name": "æ™ºèƒ½æŠ å›¾",
        "description": "æ·»åŠ èƒŒæ™¯ç§»é™¤èŠ‚ç‚¹ï¼ˆå¦‚SAMã€UÂ²-Netç­‰ï¼‰",
        "content": "- æ·»åŠ èƒŒæ™¯ç§»é™¤æˆ–æŠ å›¾èŠ‚ç‚¹\n       {\n  \"7\": {\n    \"inputs\": {\n      \"rem_mode\": \"RMBG-1.4\",\n      \"image_output\": \"Preview\",\n      \"save_prefix\": \"ComfyUI\",\n      \"torchscript_jit\": false,\n      \"add_background\": \"none\",\n      \"refine_foreground\": false\n    },\n    \"class_type\": \"easy imageRemBg\",\n    \"_meta\": {\n      \"title\": \"Image Remove Bg\"\n    }\n  }\n}\n       - æ·»åŠ SAMæŠ å›¾èŠ‚ç‚¹\n       {\n  \"8\": {\n    \"inputs\": {\n      \"prompt\": \"\",\n      \"threshold\": 0.3,\n      \"sam_model\": [\n        \"9\",\n        0\n      ],\n      \"grounding_dino_model\": [\n        \"10\",\n        0\n      ]\n    },\n    \"class_type\": \"GroundingDinoSAMSegment (segment anything)\",\n    \"_meta\": {\n      \"title\": \"GroundingDinoSAMSegment (segment anything)\"\n    }\n  },\n  \"9\": {\n    \"inputs\": {\n      \"model_name\": \"sam_vit_h (2.56GB)\"\n    },\n    \"class_type\": \"SAMModelLoader (segment anything)\",\n    \"_meta\": {\n      \"title\": \"SAMModelLoader (segment anything)\"\n    }\n  },\n  \"10\": {\n    \"inputs\": {\n      \"model_name\": \"GroundingDINO_SwinT_OGC (694MB)\"\n    },\n    \"class_type\": \"GroundingDinoModelLoader (segment anything)\",\n    \"_meta\": {\n      \"title\": \"GroundingDinoModelLoader (segment anything)\"\n    }\n  }\n}"
    },
    {
        "name": "kontextå›¾åƒç¼–è¾‘",
        "description": "æ·»åŠ kontextç³»åˆ—èŠ‚ç‚¹(è¿™ä¸ªç³»åˆ—æ˜¯æ–‡ç”Ÿå›¾å’Œå›¾ç”Ÿå›¾åœºæ™¯åº”ç”¨çš„æ¨¡å‹å’ŒèŠ‚ç‚¹,é€‚åˆç”¨é’ˆå¯¹å›¾åƒçš„èåˆï¼ŒäºŒæ¬¡ç¼–è¾‘,åŒ…æ‹¬ä½†ä¸é™äºå»æ°´å°ã€æ“¦é™¤ç‰©ä½“ã€å¤šå…ƒç´ èåˆã€å›¾ç‰‡å…ƒç´ æå–ç­‰)",
        "content": "{\n  \"6\": {\n    \"inputs\": {\n      \"text\": \"\",\n      \"clip\": [\n        \"38\",\n        0\n      ]\n    },\n    \"class_type\": \"CLIPTextEncode\",\n    \"_meta\": {\n      \"title\": \"CLIP Text Encode (Prompt)\"\n    }\n  },\n  \"8\": {\n    \"inputs\": {\n      \"samples\": [\n        \"31\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ]\n    },\n    \"class_type\": \"VAEDecode\",\n    \"_meta\": {\n      \"title\": \"VAE Decode\"\n    }\n  },\n  \"31\": {\n    \"inputs\": {\n      \"seed\": 584043043142251,\n      \"steps\": 30,\n      \"cfg\": 1.5,\n      \"sampler_name\": \"euler\",\n      \"scheduler\": \"simple\",\n      \"denoise\": 1,\n      \"model\": [\n        \"37\",\n        0\n      ],\n      \"positive\": [\n        \"35\",\n        0\n      ],\n      \"negative\": [\n        \"135\",\n        0\n      ],\n      \"latent_image\": [\n        \"124\",\n        0\n      ]\n    },\n    \"class_type\": \"KSampler\",\n    \"_meta\": {\n      \"title\": \"KSampler\"\n    }\n  },\n  \"35\": {\n    \"inputs\": {\n      \"guidance\": 2.5,\n      \"conditioning\": [\n        \"177\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxGuidance\",\n    \"_meta\": {\n      \"title\": \"FluxGuidance\"\n    }\n  },\n  \"37\": {\n    \"inputs\": {\n      \"unet_name\": \"flux1-dev-kontext_fp8_scaled.safetensors\",\n      \"weight_dtype\": \"fp8_e4m3fn\"\n    },\n    \"class_type\": \"UNETLoader\",\n    \"_meta\": {\n      \"title\": \"Load Diffusion Model\"\n    }\n  },\n  \"38\": {\n    \"inputs\": {\n      \"clip_name1\": \"clip_l.safetensors\",\n      \"clip_name2\": \"t5xxl_fp16.safetensors\",\n      \"type\": \"flux\",\n      \"device\": \"default\"\n    },\n    \"class_type\": \"DualCLIPLoader\",\n    \"_meta\": {\n      \"title\": \"DualCLIPLoader\"\n    }\n  },\n  \"39\": {\n    \"inputs\": {\n      \"vae_name\": \"ae.safetensors\"\n    },\n    \"class_type\": \"VAELoader\",\n    \"_meta\": {\n      \"title\": \"Load VAE\"\n    }\n  },\n  \"42\": {\n    \"inputs\": {\n      \"image\": [\n        \"194\",\n        0\n      ]\n    },\n    \"class_type\": \"FluxKontextImageScale\",\n    \"_meta\": {\n      \"title\": \"FluxKontextImageScale\"\n    }\n  },\n  \"124\": {\n    \"inputs\": {\n      \"pixels\": [\n        \"42\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ]\n    },\n    \"class_type\": \"VAEEncode\",\n    \"_meta\": {\n      \"title\": \"VAE Encode\"\n    }\n  },\n  \"135\": {\n    \"inputs\": {\n      \"conditioning\": [\n        \"6\",\n        0\n      ]\n    },\n    \"class_type\": \"ConditioningZeroOut\",\n    \"_meta\": {\n      \"title\": \"ConditioningZeroOut\"\n    }\n  },\n  \"177\": {\n    \"inputs\": {\n      \"conditioning\": [\n        \"6\",\n        0\n      ],\n      \"latent\": [\n        \"124\",\n        0\n      ]\n    },\n    \"class_type\": \"ReferenceLatent\",\n    \"_meta\": {\n      \"title\": \"ReferenceLatent\"\n    }\n  },\n  \"193\": {\n    \"inputs\": {\n      \"filename_prefix\": \"ComfyUI\",\n      \"images\": [\n        \"8\",\n        0\n      ]\n    },\n    \"class_type\": \"SaveImage\",\n    \"_meta\": {\n      \"title\": \"Save Image\"\n    }\n  },\n  \"194\": {\n    \"inputs\": {\n      \"image\": \"021175655C8A2A86C831C96855F3EF23.png\"\n    },\n    \"class_type\": \"LoadImage\",\n    \"_meta\": {\n      \"title\": \"Load Image\"\n    }\n  }\n}"
    },
    {
        "name": "qwen imageå›¾åƒç”Ÿæˆ",
        "description": "åŸºäºQwen-Imageå¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ–‡ç”Ÿå›¾è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒä¸­è‹±æ–‡æç¤ºè¯ï¼Œæ“…é•¿ç”ŸæˆåŒ…å«å¤æ‚æ–‡æœ¬çš„å›¾åƒå†…å®¹ã€‚é€‚ç”¨äºæµ·æŠ¥è®¾è®¡ã€å¹¿å‘Šåˆ›æ„ç­‰éœ€è¦ç¨³å®šç”Ÿæˆæ–‡å­—å†…å®¹çš„åœºæ™¯",
        "content": "æ·»åŠ qwen imageæ¨¡å‹,ä½¿ç”¨qwen image æ–‡ç”Ÿå›¾ï¼Œåˆ›ä½œä¸€å¼ æµ·æŠ¥ï¼Œé€‚åˆç”Ÿæˆç¨³å®šçš„æ–‡å­—(å½“ç”¨æˆ·æåˆ°éœ€è¦ä½¿ç”¨qwen imageç³»åˆ—æ—¶ç»™å®ƒæä¾›å¦‚ä¸‹çš„èŠ‚ç‚¹å’Œæ¨¡å‹é€‰æ‹©ï¼šload Difussion ModelèŠ‚ç‚¹å¯¹åº”qwen_imageåå­—çš„æ¨¡å‹ï¼ŒLoad CLIPå¯¹åº”çš„clip_name:qwen_2.5_vl,typeï¼šqwen_image,Load VAEèŠ‚ç‚¹å¯¹åº”çš„æ¨¡å‹ä¸º:qwen_image_vae.safetensors)"
    },
    {
        "name": "wan2.2å›¾ç”Ÿè§†é¢‘",
        "description": "wan2.2æ˜¯å›¾åƒç”Ÿæˆè§†é¢‘çš„æ¨¡å‹ï¼Œä¸»è¦ç”¨æ¥å¸®åŠ©ç”¨æˆ·æ ¹æ®å›¾ç‰‡åŠ æç¤ºè¯æ¥ç”Ÿæˆè§†é¢‘ã€‚",
        "content": "{\n  \"11\": {\n    \"inputs\": {\n      \"model_name\": \"umt5-xxl-enc-bf16.safetensors\",\n      \"precision\": \"bf16\",\n      \"load_device\": \"offload_device\",\n      \"quantization\": \"disabled\"\n    },\n    \"class_type\": \"LoadWanVideoT5TextEncoder\",\n    \"_meta\": {\n      \"title\": \"WanVideo T5 Text Encoder Loader\"\n    }\n  },\n  \"16\": {\n    \"inputs\": {\n      \"positive_prompt\": \"çŒ«å’ªçš„å¤´éƒ¨çŒ›çš„å‘å‰ä¸€æ¢\",\n      \"negative_prompt\": \"è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°\",\n      \"force_offload\": true,\n      \"use_disk_cache\": false,\n      \"device\": \"gpu\",\n      \"t5\": [\n        \"11\",\n        0\n      ]\n    },\n    \"class_type\": \"WanVideoTextEncode\",\n    \"_meta\": {\n      \"title\": \"WanVideo TextEncode\"\n    }\n  },\n  \"28\": {\n    \"inputs\": {\n      \"enable_vae_tiling\": false,\n      \"tile_x\": 272,\n      \"tile_y\": 272,\n      \"tile_stride_x\": 144,\n      \"tile_stride_y\": 128,\n      \"normalization\": \"default\",\n      \"vae\": [\n        \"38\",\n        0\n      ],\n      \"samples\": [\n        \"90\",\n        0\n      ]\n    },\n    \"class_type\": \"WanVideoDecode\",\n    \"_meta\": {\n      \"title\": \"WanVideo Decode\"\n    }\n  },\n  \"38\": {\n    \"inputs\": {\n      \"model_name\": \"Wan2_1_VAE_bf16.safetensors\",\n      \"precision\": \"bf16\"\n    },\n    \"class_type\": \"WanVideoVAELoader\",\n    \"_meta\": {\n      \"title\": \"WanVideo VAE Loader\"\n    }\n  },\n  \"39\": {\n    \"inputs\": {\n      \"blocks_to_swap\": 20,\n      \"offload_img_emb\": false,\n      \"offload_txt_emb\": false,\n      \"use_non_blocking\": false,\n      \"vace_blocks_to_swap\": 1\n    },\n    \"class_type\": \"WanVideoBlockSwap\",\n    \"_meta\": {\n      \"title\": \"WanVideo Block Swap\"\n    }\n  },\n  \"56\": {\n    \"inputs\": {\n      \"lora\": \"Wan21_I2V_14B_lightx2v_cfg_step_distill_lora_rank64.safetensors\",\n      \"strength\": 3,\n      \"low_mem_load\": true,\n      \"merge_loras\": false\n    },\n    \"class_type\": \"WanVideoLoraSelect\",\n    \"_meta\": {\n      \"title\": \"WanVideo Lora Select\"\n    }\n  },\n  \"89\": {\n    \"inputs\": {\n      \"width\": 832,\n      \"height\": 480,\n      \"num_frames\": 81,\n      \"noise_aug_strength\": 0,\n      \"start_latent_strength\": 1,\n      \"end_latent_strength\": 1,\n      \"force_offload\": true,\n      \"fun_or_fl2v_model\": false,\n      \"tiled_vae\": false,\n      \"vae\": [\n        \"38\",\n        0\n      ]\n    },\n    \"class_type\": \"WanVideoImageToVideoEncode\",\n    \"_meta\": {\n      \"title\": \"WanVideo ImageToVideo Encode\"\n    }\n  },\n  \"90\": {\n    \"inputs\": {\n      \"steps\": 8,\n      \"cfg\": 1,\n      \"shift\": 8,\n      \"seed\": 652251640766870,\n      \"force_offload\": true,\n      \"scheduler\": \"unipc\",\n      \"riflex_freq_index\": 0,\n      \"denoise_strength\": 1,\n      \"batched_cfg\": false,\n      \"rope_function\": \"comfy\",\n      \"start_step\": 4,\n      \"end_step\": -1,\n      \"image_embeds\": [\n        \"89\",\n        0\n      ],\n      \"text_embeds\": [\n        \"16\",\n        0\n      ]\n    },\n    \"class_type\": \"WanVideoSampler\",\n    \"_meta\": {\n      \"title\": \"WanVideo Sampler\"\n    }\n  },\n  \"150\": {\n    \"inputs\": {\n      \"lora\": [\n        \"56\",\n        0\n      ]\n    },\n    \"class_type\": \"WanVideoSetLoRAs\",\n    \"_meta\": {\n      \"title\": \"WanVideo Set LoRAs\"\n    }\n  }\n}"
    },
    {
        "name": "qwen_image_edit",
        "description": "qwen_image_editæ˜¯qwen_imageæ–‡ç”Ÿå›¾æ‰€å»¶å±•çš„å›¾ç‰‡ç¼–è¾‘åŠŸèƒ½çš„å·¥ä½œæµï¼Œå…¶åŠŸèƒ½ç±»ä¼¼äºflux_kontextçš„èƒ½åŠ›,ä¸»è¦åº”ç”¨åœ¨å¯¹äºå›¾ç‰‡çš„ä¿®æ”¹ã€‚å½“ç”¨æˆ·æåŠåˆ°éœ€è¦å¯¹ç”Ÿæˆå›¾è¿›è¡ŒäºŒæ¬¡ç¼–è¾‘ï¼Œä¿®æ”¹å›¾ç‰‡å†…å®¹ã€æ”¹å˜å›¾ç‰‡ä¸­çš„æŸäº›å…ƒç´ ã€æ ¹æ®å›¾ç‰‡å›¾ç‰‡ä¸­çš„æŸäº›ç‰¹å¾è¿›è¡ŒäºŒæ¬¡å»¶å±•æ—¶ï¼Œä¼˜å…ˆæ¨èè¯¥ç±»å·¥ä½œæµã€‚",
        "content": "ç‰¹åˆ«æ³¨æ„qwen_image_deitæœ‰ç‰¹å®šçš„ä¸€å¥—\"difussion model\"å’Œ\"clip\"ã€\"vae\"æ¨¡å‹ï¼Œåˆ†åˆ«æ˜¯\"difussion model:qwen_image_edit.safetensor\"ã€\"clip:qwen_2.5vl_7b_fp8_scaled.safetensors\"å’Œ\"qwen_image type\",\"vae:qwen_image_vae\"ã€‚å¦å¤–ï¼Œå¯¹äºqwen_image_editçš„æ–‡æœ¬è¾“å…¥èŠ‚ç‚¹éœ€è¦ä½¿ç”¨ä¸“å±çš„\"textencoedqwenimageedit\"é™¤æ­¤ä¹‹å¤–è¯¥ç±»å·¥ä½œæµä¸ä¸ç”»å¸ƒä¸Šå…¶ä»–çš„modelå…±ç”¨éœ€å•ç‹¬æ·»åŠ ä¸Šè¿°ä¸“å±èŠ‚ç‚¹å’Œmodelã€‚ {\n  \"37\": {\n    \"inputs\": {\n      \"unet_name\": \"qwen_image_edit_fp8_e4m3fn.safetensors\",\n      \"weight_dtype\": \"default\"\n    },\n    \"class_type\": \"UNETLoader\",\n    \"_meta\": {\n      \"title\": \"Load Diffusion Model\"\n    }\n  },\n  \"38\": {\n    \"inputs\": {\n      \"clip_name\": \"qwen_2.5_vl_7b_fp8_scaled.safetensors\",\n      \"type\": \"qwen_image\",\n      \"device\": \"default\"\n    },\n    \"class_type\": \"CLIPLoader\",\n    \"_meta\": {\n      \"title\": \"Load CLIP\"\n    }\n  },\n  \"39\": {\n    \"inputs\": {\n      \"vae_name\": \"qwen_image_vae.safetensors\"\n    },\n    \"class_type\": \"VAELoader\",\n    \"_meta\": {\n      \"title\": \"Load VAE\"\n    }\n  },\n  \"66\": {\n    \"inputs\": {\n      \"shift\": 3,\n      \"model\": [\n        \"37\",\n        0\n      ]\n    },\n    \"class_type\": \"ModelSamplingAuraFlow\",\n    \"_meta\": {\n      \"title\": \"ModelSamplingAuraFlow\"\n    }\n  },\n  \"75\": {\n    \"inputs\": {\n      \"strength\": 1,\n      \"model\": [\n        \"66\",\n        0\n      ]\n    },\n    \"class_type\": \"CFGNorm\",\n    \"_meta\": {\n      \"title\": \"CFGNorm\"\n    }\n  },\n  \"76\": {\n    \"inputs\": {\n      \"prompt\": \"\",\n      \"clip\": [\n        \"38\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ],\n      \"image\": [\n        \"93\",\n        0\n      ]\n    },\n    \"class_type\": \"TextEncodeQwenImageEdit\",\n    \"_meta\": {\n      \"title\": \"TextEncodeQwenImageEdit\"\n    }\n  },\n  \"77\": {\n    \"inputs\": {\n      \"prompt\": \"\",\n      \"clip\": [\n        \"38\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ],\n      \"image\": [\n        \"93\",\n        0\n      ]\n    },\n    \"class_type\": \"TextEncodeQwenImageEdit\",\n    \"_meta\": {\n      \"title\": \"TextEncodeQwenImageEdit\"\n    }\n  },\n  \"88\": {\n    \"inputs\": {\n      \"pixels\": [\n        \"93\",\n        0\n      ],\n      \"vae\": [\n        \"39\",\n        0\n      ]\n    },\n    \"class_type\": \"VAEEncode\",\n    \"_meta\": {\n      \"title\": \"VAE Encode\"\n    }\n  },\n  \"93\": {\n    \"inputs\": {\n      \"upscale_method\": \"lanczos\",\n      \"megapixels\": 1\n    },\n    \"class_type\": \"ImageScaleToTotalPixels\",\n    \"_meta\": {\n      \"title\": \"Scale Image to Total Pixels\"\n    }\n  }\n}"
    }
]